# -*- coding: utf-8 -*-
"""CNN_classification_기본구현_LYJ.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zw04tXbXewTALjOTaiQKp361z0eJ3Hyk

- 코딩 목표: pytorch 이용해서 모든 데이터에 이용할 수 있는 기본 CNN classification 모델 구현. model은 vgg, resnet 상관없이 모두 이용할 수 있도록.

## 커스텀 데이터셋
pytorch 에서 제공하는 툴을 사용하여 커스텀 데이터를 만들어 사용하면 미니 배치 학습, 데이터 셔플, 병렬처리등을 간단히 수행가능. Dataset을 정의하고, 이를 DataLoader에 전달하는 것이 기본 방법.
"""

import pandas as pd
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torchvision
from torchvision import datasets, transforms, models
import os
import pickle
import sys
# 3rd-party Modules
import time
from tqdm.auto import tqdm
from PIL import Image
# Pytorch Modules
from torch.utils.data.dataset import Dataset
from torchvision.datasets import load_dataset
import logging
from torch.optim.lr_scheduler import StepLR, LambdalR, CosineAnnealingLR, CosineAnnealingWarmResarts,ReduceLROnPlateau

import easydict #colab 환경에서 argparser 비슷하게

"""# dataset 구성요소

class my_dataset(torch.utils.data.Dataset):

    def __init__(self, x, transforms=None):
    #데이터셋의 전처리를 해주는 부분
    
    def __len__(self):
    #데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분
    
    def __getitem__(self, idx):
    #데이터셋에서 특정 1개의 샘플을 가져오는 함수

# Data

데이터 셋 다시 다운받기

matplot.lib
이미지 볼 수 있음

참고 사이트:Custom dataset based on CIFAR10
https://discuss.pytorch.org/t/custom-dataset-based-on-cifar10/171136

## Data preprocessing & Custom data
"""

from google.colab import drive
drive.mount('/content/drive')

#모델 별 argparse

user_name=Yoonji
proj_name=cnn_classification

args=easydict.EasyDict({
    "task":"classification",
    "task_dataset":'mnist',
    "train_valid_split":0.1,
    "process_path":f'./CNN_classification/preprocessed/{proj_name}',
    "model_type":'vit_b_16', #model_type_list = ['vgg11', 'resnet50', 'resnet152', 'efficientnet_b0', 'efficientnet_b7', 'vit_b_16']
    "image_resize_size":224,
    "dropout_rate":0.2,
    "optimizer":'Adam', #optim_list = ['SGD', 'AdaDelta', 'Adam', 'AdamW'],
    "scheduler":'None' #scheduler_list = ['None', 'StepLR', 'LambdaLR', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts', 'ReduceLROnPlateau'],
    "batch_size":32,
    "weight_dacay":0,
    "clip_grad_norm":5,
    "optimize_objective":'accuracy' #objective_list = ['loss', 'accuracy', 'f1'],
    "device":'cuda',
    "seed":2023,
    "use_tensorboard":True,
    "use_wandb":Truelog_freq:500,
    "num_workers":2  #number of CPU workers,
    "num_epochs":50,
    "learning_rate":5e-5

})

#utils
def check_path(path:str):
  if not os.path.exists(path):
    os.makedirs(path)

def set_random_seed(seed:int):
  torch.manual_seed_all(seed)
  np.random.seed(seed)
  random.seed(seed)

def get_torch_device(device:str):
  if device is not None:
    get_torch_device.device=device

  if 'cuda' in get_torch_device.device:
    if torch.cuda.is_available():
      return torch.device(get_torch_device.get_torch_device)
    else:
      print("No GPU found. Using CPU.")
      return torch.device('cpu')

class TqdmLoggingHandler(logging.Handler):
    def __init__(self, level=logging.DEBUG):
        super().__init__(level)
        self.stream = sys.stdout

    def flush(self):
        self.acquire()
        try:
            if self.stream and hasattr(self.stream, "flush"):
                self.stream.flush()
        finally:
            self.release()

    def emit(self, record):
        try:
            msg = self.format(record)
            tqdm.tqdm.write(msg, self.stream)
            self.flush()
        except (KeyboardInterrupt, SystemExit, RecursionError):
            raise
        except Exception:
            self.handleError(record)

def write_log(logger, message):
    if logger:
        logger.info(message)

def get_tb_exp_name(args):
    """
    Get the experiment name for tensorboard experiment.
    """

    ts = time.strftime('%Y-%b-%d-%H:%M:%S', time.localtime())

    exp_name = str()
    exp_name += "%s - " % args.task.upper()
    exp_name += "%s - " % proj_name

    if args.job in ['training', 'resume_training']:
        exp_name += 'TRAIN - '
        exp_name += "MODEL=%s - " % args.model_type.upper()
        exp_name += "DATA=%s - " % args.task_dataset.upper()
        exp_name += "DESC=%s - " % args.description
    elif args.job == 'testing':
        exp_name += 'TEST - '
        exp_name += "MODEL=%s - " % args.model_type.upper()
        exp_name += "DATA=%s - " % args.task_dataset.upper()
        exp_name += "DESC=%s - " % args.description
    exp_name += "TS=%s" % ts

    return exp_name

def get_wandb_exp_name(args: argparse.Namespace):
    """
    Get the experiment name for weight and biases experiment.
    """

    exp_name = str()
    exp_name += "%s - " % args.task.upper()
    exp_name += "%s / " % args.task_dataset.upper()
    exp_name += "%s" % args.model_type.upper()

    return exp_name

def get_huggingface_model_name(model_type: str) -> str:
    name = model_type.lower()

    if name in ['bert', 'cnn', 'lstm', 'gru', 'rnn', 'transformer_enc']: # 'cnn' and 'lstm' shares bert tokenizer.
        return 'bert-base-uncased'
    elif name == 'bart':
        return 'facebook/bart-large'
    elif name == 't5':
        return 't5-large'
    elif name == 'roberta':
        return 'roberta-base'
    elif name == 'electra':
        return 'google/electra-base-discriminator'
    elif name == 'albert':
        return 'albert-base-v2'
    elif name == 'deberta':
        return 'microsoft/deberta-base'
    elif name == 'debertav3':
        return 'microsoft/deberta-v3-base'
    else:
        raise NotImplementedError

# Model
class ClassificationModel(nn.Module):
  def __init(self, args.model_type)->None:
    super(ClassificationModel,self).__init__()

    if args.model_type == 'vgg11':
            self.model = torchvision.models.vgg11(weights=torchvision.models.VGG11_Weights.IMAGENET1K_V1)
            args.image_resize_size = 224 # VGG11 requires 224x224 image size

            self.feature_extractor = nn.Sequential(*list(self.model.children())[:-1])
            self.feature_output_size = self.model.classifier[0].in_features
        elif args.model_type == 'resnet50':
            self.model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V1)
            args.image_resize_size = 224 # ResNet50 requires 224x224 image size

            self.feature_extractor = nn.Sequential(*list(self.model.children())[:-1])
            self.feature_output_size = self.model.fc.in_features
        elif args.model_type == 'resnet152':
            self.model = torchvision.models.resnet152(weights=torchvision.models.ResNet152_Weights.IMAGENET1K_V1)
            args.image_resize_size = 224 # ResNet152 requires 224x224 image size

            self.feature_extractor = nn.Sequential(*list(self.model.children())[:-1])
            self.feature_output_size = self.model.fc.in_features
        elif args.model_type == 'efficientnet_b0':
            self.model = torchvision.models.efficientnet_b0(weights=torchvision.models.EfficientNet_B0_Weights.IMAGENET1K_V1)
            args.image_resize_size = 224 # EfficientNet-B0 requires 224x224 image size

            self.feature_extractor = nn.Sequential(*list(self.model.children())[:-1])
            self.feature_output_size = self.model.classifier[1].in_features
        elif args.model_type == 'efficientnet_b7':
            self.model = torchvision.models.efficientnet_b7(weights=torchvision.models.EfficientNet_B7_Weights.IMAGENET1K_V1)
            args.image_resize_size = 600 # EfficientNet-B7 requires 600x600 image size

            self.feature_extractor = nn.Sequential(*list(self.model.children())[:-1])
            self.feature_output_size = self.model.classifier[1].in_features
        elif args.model_type == 'vit_b_16':
            self.model = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1)
            args.image_resize_size = 224 # ViT-B/16 requires 224x224 image size

            self.feature_extractor = nn.Sequential(*list(self.model.children())[:-1])
            self.feature_output_size = self.model.heads[0].in_features
        else:
            raise NotImplementedError(f'Invalid model type: {args.model_type}')

        # Define the final classification layer
        self.classifier = nn.Sequential(
            nn.Linear(self.feature_output_size, self.feature_output_size // 2),
            nn.Dropout(self.args.dropout_rate),
            nn.ReLU(),
            nn.Linear(self.feature_output_size // 2, args.num_classes),
        )

    def forward(self, images: torch.Tensor) -> torch.Tensor:
        if 'vit' in self.args.model_type:
            # vit requires preprocessing
            encoder = self.feature_extractor[1]
            processed_img = self.model._process_input(images)

            n = processed_img.size(0)
            # Expand the class token to the full batch
            batch_class_token = self.model.class_token.expand(n, -1, -1)
            processed_img = torch.cat([batch_class_token, processed_img], dim=1)

            encoded_img = encoder(processed_img)
            features = encoded_img[:, 0]
        elif self.args.model_type in ['vgg11', 'resnet50', 'resnet152', 'efficientnet_b0', 'efficientnet_b7']:
            features = self.feature_extractor(images)

        features = features.view(features.size(0), -1) # Flatten the features to (batch_size, feature_output_size)
        logits = self.classifier(features)

        return logits




# optimizer
def get optimizer(model:nn.Module, learning_rate:float=None, weight_decay:float=None,
                  optim_type:str=None)-> torch.optim.Optimizer:
                  if learning_rate is None:
                    if args is None:
                      raise ValueError('Either learning_rate or args must be given.')
                  else:
                    learning_rate = arg.learning_rate
                  if weight_decay is None:
        if args is None:
            raise ValueError('Either weight_decay or args must be given.')
        else:
            weight_decay = args.weight_decay
    if optim_type is None:
        if args is None:
            raise ValueError('Either optim_type or args must be given.')
        else:
            optim_type = args.optimizer

    if weight_decay > 0:
        if optim_type == 'SGD':
            return torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
        elif optim_type == 'AdaDelta':
            return torch.optim.Adadelta(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
        elif optim_type == 'Adam':
            return torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
        elif optim_type == 'AdamW':
            return torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
        else:
            raise ValueError(f'Unknown optimizer option {optim_type}')
    else:
        if optim_type == 'SGD':
            return torch.optim.SGD(model.parameters(), lr=learning_rate)
        elif optim_type == 'Adam':
            return torch.optim.Adam(model.parameters(), lr=learning_rate)
        elif optim_type == 'AdamW':
            return torch.optim.AdamW(model.parameters(), lr=learning_rate)
        else:
            raise ValueError(f'Unknown optimizer option {optim_type}')


# Get_scheduler
def get_scheduler(optimizer:torch.optim.Optimizer,dataloader_length: int,
                  num_epochs: int=None, early_stopping_patience: int=None, learning_rate: float=None,
                  scheduler_type: str=None)-> torch.optim.lr_scheduler:

########################
## Data preprocessing ##
########################

def load_data():

    name = args.task_dataset.lower()
    train_valid_split = args.train_valid_split


    train_data = {
        'image': [],
        'label': []
    }
    valid_data = {
        'image': [],
        'label': []
    }
    test_data = {
        'image': [],
        'label': []
    }

    if name == 'mnist':
        dataset = load_dataset('mnist')

        train_df = pd.DataFrame(dataset['train'])
        #valid_df = pd.DataFrame(dataset['validation']) # No pre-defined validation set
        test_df = pd.DataFrame(dataset['test'])
        num_classes = 10

        # train-valid split
        train_df = train_df.sample(frac=1).reset_index(drop=True)
        valid_df = train_df[:int(len(train_df) * train_valid_split)]
        train_df = train_df[int(len(train_df) * train_valid_split):]

        train_data['image'] = train_df['image'].tolist()
        train_data['label'] = train_df['label'].tolist()
        valid_data['image'] = valid_df['image'].tolist()
        valid_data['label'] = valid_df['label'].tolist()
        test_data['image'] = test_df['image'].tolist()
        test_data['label'] = test_df['label'].tolist()
    elif name == 'fashion_mnist':
        dataset = load_dataset('fashion_mnist')

        train_df = pd.DataFrame(dataset['train'])
        #valid_df = pd.DataFrame(dataset['validation']) # No pre-defined validation set
        test_df = pd.DataFrame(dataset['test'])
        num_classes = 10

        # train-valid split
        train_df = train_df.sample(frac=1).reset_index(drop=True)
        valid_df = train_df[:int(len(train_df) * train_valid_split)]
        train_df = train_df[int(len(train_df) * train_valid_split):]

        train_data['image'] = train_df['image'].tolist()
        train_data['label'] = train_df['label'].tolist()
        valid_data['image'] = valid_df['image'].tolist()
        valid_data['label'] = valid_df['label'].tolist()
        test_data['image'] = test_df['image'].tolist()
        test_data['label'] = test_df['label'].tolist()
    elif name == 'cifar10':
        dataset = load_dataset('cifar10')

        train_df = pd.DataFrame(dataset['train'])
        #valid_df = pd.DataFrame(dataset['validation']) # No pre-defined validation set
        test_df = pd.DataFrame(dataset['test'])
        num_classes = 10

        # train-valid split
        train_df = train_df.sample(frac=1).reset_index(drop=True)
        valid_df = train_df[:int(len(train_df) * train_valid_split)]
        train_df = train_df[int(len(train_df) * train_valid_split):]

        train_data['image'] = train_df['img'].tolist()
        train_data['label'] = train_df['label'].tolist()
        valid_data['image'] = valid_df['img'].tolist()
        valid_data['label'] = valid_df['label'].tolist()
        test_data['image'] = test_df['img'].tolist()
        test_data['label'] = test_df['label'].tolist()
    elif name == 'cifar100':
        dataset = load_dataset('cifar100')

        train_df = pd.DataFrame(dataset['train'])
        #valid_df = pd.DataFrame(dataset['validation']) # No pre-defined validation set
        test_df = pd.DataFrame(dataset['test'])
        num_classes = 100

        # train-valid split
        train_df = train_df.sample(frac=1).reset_index(drop=True)
        valid_df = train_df[:int(len(train_df) * train_valid_split)]
        train_df = train_df[int(len(train_df) * train_valid_split):]

        train_data['image'] = train_df['img'].tolist()
        train_data['label'] = train_df['fine_label'].tolist()
        valid_data['image'] = valid_df['img'].tolist()
        valid_data['label'] = valid_df['fine_label'].tolist()
        test_data['image'] = test_df['img'].tolist()
        test_data['label'] = test_df['fine_label'].tolist()

    return train_data, valid_data, test_data, num_classes

def preprocessing(args: argparse.Namespace) -> None:

    # Load data
    train_data, valid_data, test_data, num_classes = load_data(args)

    # Preprocessing - Define data_dict
    data_dict = {
        'train': {
            'images': train_data['image'],
            'labels': train_data['label'],
            'num_classes': num_classes,
        },
        'valid': {
            'images': valid_data['image'],
            'labels': valid_data['label'],
            'num_classes': num_classes,
        },
        'test': {
            'images': test_data['image'],
            'labels': test_data['label'],
            'num_classes': num_classes,
        }
    }
# Save data as pickle file
    preprocessed_path = os.path.join(args.preprocess_path, args.task, args.task_dataset, args.model_type)
    check_path(preprocessed_path)

    for split_data, split in zip([train_data, valid_data, test_data], ['train', 'valid', 'test']):
        # Save data as pickle file
        with open(os.path.join(preprocessed_path, f'{split}_processed.pkl'), 'wb') as f:
            pickle.dump(data_dict[split], f)

########################
##  Custom Dataset    ##
########################

# class 안에서 img, label
class custom_Dataset(torch.utils.data.Dataset):
  def __init__(self, data_path:str) -> None:
    super(custom_Dataset,self).__init__()
    with open(data_path, 'rb') as f:
      data_ = pickle.load(f)

    self.data_list=[]
    self.num_classes=data_['num_classes']

    self.transform=transforms.Compose([
        transforms.ToTensor(),
        transforms.Resize(args.image_resize_size),
        transforms.CenterCrop(224),transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])

    for idx in tqdm(range(len(data_['images'])), desc=f'Loading data from {data_path}'):
      PIL_images = data_['images'][idx].convert('RGB')

      self.data_list.append({
          'images':PIL_images,
          'labels':data_['labels'][idx],
          'index':idx
      })

    del data_


    self.data=datasets.CIFAR10('data', transform=transform,download=True)
    # resnet 모델 기준에 맞춰서 transform

  def __getitem__(self, idx) -> dict:
    torch_images = self.transform(self.data_list[idx]['images'])
    item_dict = {
        'images':torch_images,
        'labels':self.data_list[idx]['labels'],
        'index':idx
    }
    return item_dict


  def __len__(self) -> int:
    return len(self.data_list)
  #데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분

  def collate_fn(data):
    image = torch.stack([d['images'] for d in data])
    label = torch.tensor([d['labels'] for d in data], dtype=torch.long)
    indices = torch.tensor([d['index'] for d in data], dtype=torch.long)

    datas_dict={
        'images': image,
        'labels': label,
        'index' : indices
    }
    return datas_dict

"""#Model training

참고 사이트:
https://github.com/CryptoSalamander/pytorch_paper_implementation/blob/master/resnet/resnet_cifar10.ipynb
"""

import torch.nn as nn
import torchvision.models as models
import torch.optim as optim

collate_fn=custom_Dataset().collate_fn

dataset_dict, dataloader_dict = {}, {}
dataset_dict['train'] = custom_Dataset(os.path.join(args.preprocess_path, args.task, args.task_dataset, args.model_type, f'train_processed.pkl'))
dataset_dict['valid'] = custom_Dataset(os.path.join(args.preprocess_path, args.task, args.task_dataset, args.model_type, f'valid_processed.pkl'))

dataloader_dict['train'] = DataLoader(dataset_dict['train'], batch_size=args.batch_size, num_workers=args.num_workers,
                                          shuffle=True, pin_memory=True, drop_last=True, collate_fn=collate_fn)
dataloader_dict['valid'] = DataLoader(dataset_dict['valid'], batch_size=args.batch_size, num_workers=args.num_workers,
                                          shuffle=False, pin_memory=True, drop_last=False, collate_fn=collate_fn)
args.num_classes = dataset_dict['train'].num_classes

model=ClassificationModel(args.model_type).to(device)

optimizer = get_optimizer(model, learning_rate=args.learning_rate, weight_decay=args.weight_decay, optim_type=args.optimizer)
cls_loss = nn.CrossEntropyLoss(label_smoothing=args.label_smoothing_eps)

learning_rate = 0.1
num_epoch = 50
model_name = 'res_model.pth'

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(model_res.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)

train_loss = 0
valid_loss = 0
correct = 0
total_cnt = 0
best_acc = 0

# Train/Valid - Start training
best_epoch_idx = 0
best_valid_objective_value = None
early_stopping_counter = 0


for epoch in range(args.num_epoch):
    model=model.train()
    train_loss_cls=0
    train_acc_cls=0
    train_f1_cls=0


    # Train Phase
    for iter_idx, data_dicts in enumerate(tqdm(dataloader_dict['train'], total=len(dataloader_dict['train']), desc=f'Training - Epoch [{epoch_idx}/{args.num_epochs}]', position=0, leave=True)):
            # Train - Get input data
            images = data_dicts['images'].to(device)
            labels = data_dicts['labels'].to(device)

            # Train - Forward pass
            classification_logits = model(images)

            # Train - Calculate loss & accuracy/f1 score
            batch_loss_cls = cls_loss(classification_logits, labels)
            batch_acc_cls = (classification_logits.argmax(dim=-1) == labels).float().mean()
            batch_f1_cls = f1_score(labels.cpu().numpy(), classification_logits.argmax(dim=-1).cpu().numpy(), average='macro')

            # Train - Backward pass
            optimizer.zero_grad()
            batch_loss_cls.backward()
            if args.clip_grad_norm > 0:
                nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad_norm)
            optimizer.step()
            if args.scheduler in ['StepLR', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']:
                scheduler.step() # These schedulers require step() after every training iteration

            # Train - Logging
            train_loss_cls += batch_loss_cls.item()
            train_acc_cls += batch_acc_cls.item()
            train_f1_cls += batch_f1_cls

            if iter_idx % args.log_freq == 0 or iter_idx == len(dataloader_dict['train']) - 1:
                write_log(logger, f"TRAIN - Epoch [{epoch_idx}/{args.num_epochs}] - Iter [{iter_idx}/{len(dataloader_dict['train'])}] - Loss: {batch_loss_cls.item():.4f}")
                write_log(logger, f"TRAIN - Epoch [{epoch_idx}/{args.num_epochs}] - Iter [{iter_idx}/{len(dataloader_dict['train'])}] - Acc: {batch_acc_cls.item():.4f}")
                write_log(logger, f"TRAIN - Epoch [{epoch_idx}/{args.num_epochs}] - Iter [{iter_idx}/{len(dataloader_dict['train'])}] - F1: {batch_f1_cls:.4f}")
            if args.use_tensorboard:
                writer.add_scalar('TRAIN/Learning_Rate', optimizer.param_groups[0]['lr'], epoch_idx * len(dataloader_dict['train']) + iter_idx)

        # Train - End of epoch logging
        if args.use_tensorboard:
            writer.add_scalar('TRAIN/Loss', train_loss_cls / len(dataloader_dict['train']), epoch_idx)
            writer.add_scalar('TRAIN/Acc', train_acc_cls / len(dataloader_dict['train']), epoch_idx)
            writer.add_scalar('TRAIN/F1', train_f1_cls / len(dataloader_dict['train']), epoch_idx)

        # Valid - Set model to eval mode
        model = model.eval()
        valid_loss_cls = 0
        valid_acc_cls = 0
        valid_f1_cls = 0

        # Valid - Iterate one epoch
        for iter_idx, data_dicts in enumerate(tqdm(dataloader_dict['valid'], total=len(dataloader_dict['valid']), desc=f'Validating - Epoch [{epoch_idx}/{args.num_epochs}]', position=0, leave=True)):
            # Valid - Get input data
            images = data_dicts['images'].to(device)
            labels = data_dicts['labels'].to(device)

            # Valid - Forward pass
            with torch.no_grad():
                classification_logits = model(images)

            # Valid - Calculate loss & accuracy/f1 score
            batch_loss_cls = cls_loss(classification_logits, labels)
            batch_acc_cls = (classification_logits.argmax(dim=-1) == labels).float().mean()
            batch_f1_cls = f1_score(labels.cpu().numpy(), classification_logits.argmax(dim=-1).cpu().numpy(), average='macro')

            # Valid - Logging
            valid_loss_cls += batch_loss_cls.item()
            valid_acc_cls += batch_acc_cls.item()
            valid_f1_cls += batch_f1_cls

            if iter_idx % args.log_freq == 0 or iter_idx == len(dataloader_dict['valid']) - 1:
                write_log(logger, f"VALID - Epoch [{epoch_idx}/{args.num_epochs}] - Iter [{iter_idx}/{len(dataloader_dict['valid'])}] - Loss: {batch_loss_cls.item():.4f}")
                write_log(logger, f"VALID - Epoch [{epoch_idx}/{args.num_epochs}] - Iter [{iter_idx}/{len(dataloader_dict['valid'])}] - Acc: {batch_acc_cls.item():.4f}")
                write_log(logger, f"VALID - Epoch [{epoch_idx}/{args.num_epochs}] - Iter [{iter_idx}/{len(dataloader_dict['valid'])}] - F1: {batch_f1_cls:.4f}")

        # Valid - Call scheduler
        if args.scheduler == 'LambdaLR':
            scheduler.step()
        elif args.scheduler == 'ReduceLROnPlateau':
            scheduler.step(valid_loss_cls)

        # Valid - Check loss & save model
        valid_loss_cls /= len(dataloader_dict['valid'])
        valid_acc_cls /= len(dataloader_dict['valid'])
        valid_f1_cls /= len(dataloader_dict['valid'])

        if args.optimize_objective == 'loss':
            valid_objective_value = valid_loss_cls
            valid_objective_value = -1 * valid_objective_value # Loss is minimized, but we want to maximize the objective value
        elif args.optimize_objective == 'accuracy':
            valid_objective_value = valid_acc_cls
        elif args.optimize_objective == 'f1':
            valid_objective_value = valid_f1_cls
        else:
            raise NotImplementedError

        if best_valid_objective_value is None or valid_objective_value > best_valid_objective_value:
            best_valid_objective_value = valid_objective_value
            best_epoch_idx = epoch_idx
            write_log(logger, f"VALID - Saving checkpoint for best valid {args.optimize_objective}...")
            early_stopping_counter = 0 # Reset early stopping counter

            checkpoint_save_path = os.path.join(args.checkpoint_path, args.task, args.task_dataset, args.model_type)
            check_path(checkpoint_save_path)

            torch.save({
                'epoch': epoch_idx,
                'model': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'scheduler': scheduler.state_dict() if scheduler is not None else None
            }, os.path.join(checkpoint_save_path, f'checkpoint.pt'))
            write_log(logger, f"VALID - Best valid at epoch {best_epoch_idx} - {args.optimize_objective}: {abs(best_valid_objective_value):.4f}")
            write_log(logger, f"VALID - Saved checkpoint to {checkpoint_save_path}")
        else:
            early_stopping_counter += 1
            write_log(logger, f"VALID - Early stopping counter: {early_stopping_counter}/{args.early_stopping_patience}")

        # Valid - End of epoch logging
        if args.use_tensorboard:
            writer.add_scalar('VALID/Loss', valid_loss_cls, epoch_idx)
            writer.add_scalar('VALID/Acc', valid_acc_cls, epoch_idx)
            writer.add_scalar('VALID/F1', valid_f1_cls, epoch_idx)
        if args.use_wandb:
            wandb.log({'TRAIN/Epoch_Loss': train_loss_cls / len(dataloader_dict['train']),
                       'TRAIN/Epoch_Acc': train_acc_cls / len(dataloader_dict['train']),
                       'TRAIN/Epoch_F1': train_f1_cls / len(dataloader_dict['train']),
                       'VALID/Epoch_Loss': valid_loss_cls,
                       'VALID/Epoch_Acc': valid_acc_cls,
                       'VALID/Epoch_F1': valid_f1_cls,
                       'Epoch_Index': epoch_idx})
            wandb.alert(
                title='Epoch End',
                text=f"VALID - Epoch {epoch_idx} - Loss: {valid_loss_cls:.4f} - Acc: {valid_acc_cls:.4f}",
                level=AlertLevel.INFO,
                wait_duration=300
            )

        # Valid - Early stopping
        if early_stopping_counter >= args.early_stopping_patience:
            write_log(logger, f"VALID - Early stopping at epoch {epoch_idx}...")
            break

    # Final - End of training
    write_log(logger, f"Done! Best valid at epoch {best_epoch_idx} - {args.optimize_objective}: {abs(best_valid_objective_value):.4f}")
    if args.use_tensorboard:
        writer.add_text('VALID/Best', f"Best valid at epoch {best_epoch_idx} - {args.optimize_objective}: {abs(best_valid_objective_value):.4f}")
        writer.close()

    # Final - Save best checkpoint as result model
    final_model_save_path = os.path.join(args.model_path, args.task, args.task_dataset, args.model_type)
    check_path(final_model_save_path)
    shutil.copyfile(os.path.join(checkpoint_save_path, 'checkpoint.pt'), os.path.join(final_model_save_path, 'final_model.pt')) # Copy best checkpoint as final model
    write_log(logger, f"FINAL - Saved final model to {final_model_save_path}")

    if args.use_wandb:
        wandb.finish()





